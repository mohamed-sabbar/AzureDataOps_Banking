trigger:
  branches:
    include:
      - main

variables:
  DATABRICKS_WORKSPACE_URL: "https://adb-xxxx.xx.azuredatabricks.net"
  DATABRICKS_TOKEN: "$(databricksToken)"     # Stocké dans Azure DevOps Library
  NOTEBOOKS_PATH: "/Repos/mon-projet/notebooks"
  LOCAL_PATH: "notebooks/"

stages:

# ──────────────────────
# 1️⃣ CONTINUOUS INTEGRATION
# ──────────────────────
- stage: CI
  displayName: "CI - Validation du code"
  jobs:
    - job: Validate
      steps:

        - checkout: self

        - task: UsePythonVersion@0
          inputs:
            versionSpec: '3.10'

        - script: |
            pip install black flake8 databricks-cli
            black --check .
            flake8 .
          displayName: "Validation du style Python"

# ──────────────────────
# 2️⃣ CONTINUOUS DELIVERY (Deployment vers Databricks)
# ──────────────────────
- stage: CD
  displayName: "CD - Deploy vers Azure Databricks"
  dependsOn: CI
  jobs:
    - job: Deploy
      steps:

        - checkout: self

        # Login à Databricks
        - script: |
            databricks configure --token <<EOF
            $(DATABRICKS_WORKSPACE_URL)
            $(DATABRICKS_TOKEN)
            EOF
          displayName: "Databricks CLI - Login"

        # Synchronisation du code vers Databricks Repos
        - script: |
            databricks repos update \
              --path "$(NOTEBOOKS_PATH)" \
              --branch "main"
          displayName: "Sync Repos"

        # Upload manuel (si tu n’utilises pas Repos)
        - script: |
            databricks workspace import_dir \
              $(LOCAL_PATH) $(NOTEBOOKS_PATH) \
              --overwrite
          displayName: "Upload notebooks → Workspace"

        # (Optionnel) Exécuter un notebook de validation
        - script: |
            RUN_ID=$(databricks jobs run-now --job-id 1234 | jq -r '.run_id')
            echo "Run ID: $RUN_ID"
          displayName: "Exécution job de validation"
