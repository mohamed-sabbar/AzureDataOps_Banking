# Pipeline CI/CD amÃ©liorÃ© - Azure DataOps Platform
# BasÃ© sur votre fichier existant + bonnes pratiques CI/CD

trigger:
  branches:
    include:
      - main
      - develop
  paths:
    include:
      - notebooks/**

pool:
  vmImage: 'ubuntu-latest'

variables:
  PYTHON_VERSION: '3.10'

stages:
  # ==========================================
  # STAGE 1 : CI - VALIDATION DES NOTEBOOKS
  # ==========================================
  - stage: CI_Validation
    displayName: 'ğŸ” CI - Validation'
    jobs:
      - job: ValidateNotebooks
        displayName: 'Valider notebooks'
        steps:
          
          # Debug structure
          - script: |
              echo "ğŸ“‚ STRUCTURE DU REPO :"
              ls -R
              echo ""
              echo "ğŸ“ Contenu de notebooks/ :"
              ls -la notebooks/ || echo "âŒ Le dossier notebooks n'existe pas"
            displayName: 'ğŸ” Debug - Structure du repo'
          
          # Python
          - task: UsePythonVersion@0
            inputs:
              versionSpec: '$(PYTHON_VERSION)'
            displayName: 'ğŸ Python $(PYTHON_VERSION)'
          
          # DÃ©pendances
          - script: |
              pip install databricks-cli flake8 pytest
              if [ -f "requirements.txt" ]; then
                pip install -r requirements.txt
              fi
            displayName: 'ğŸ“¦ Installer dÃ©pendances'
          
          # Validation syntaxe Python
          - script: |
              echo "ğŸ” Validation de la syntaxe des notebooks..."
              
              if [ ! -d "notebooks" ]; then
                echo "âŒ ERREUR : Le dossier notebooks n'existe pas"
                exit 1
              fi
              
              has_errors=0
              for notebook in notebooks/*.py notebooks/*.ipynb; do
                if [ -f "$notebook" ]; then
                  echo "âœ“ VÃ©rification : $(basename $notebook)"
                  
                  # Pour les .py
                  if [[ "$notebook" == *.py ]]; then
                    python -m py_compile "$notebook" || has_errors=1
                  fi
                fi
              done
              
              if [ $has_errors -eq 0 ]; then
                echo "âœ… Tous les notebooks sont valides"
              else
                echo "âŒ Des erreurs ont Ã©tÃ© dÃ©tectÃ©es"
                exit 1
              fi
            displayName: 'ğŸ” Valider syntaxe'
          
          # Linting optionnel
          - script: |
              echo "ğŸ“‹ Analyse qualitÃ© du code..."
              flake8 notebooks/ --max-line-length=120 --ignore=E402,W503,E501 || true
            displayName: 'ğŸ“‹ Linting (optionnel)'
            continueOnError: true
          
          # CrÃ©er artifact
          - task: ArchiveFiles@2
            displayName: 'ğŸ“¦ CrÃ©er artifact'
            inputs:
              rootFolderOrFile: '$(Build.SourcesDirectory)/notebooks'
              includeRootFolder: true
              archiveType: 'zip'
              archiveFile: '$(Build.ArtifactStagingDirectory)/notebooks-$(Build.BuildNumber).zip'
          
          - task: PublishBuildArtifacts@1
            displayName: 'ğŸ“¤ Publier artifact'
            inputs:
              pathToPublish: '$(Build.ArtifactStagingDirectory)'
              artifactName: 'notebooks'

  # ==========================================
  # STAGE 2 : DÃ‰PLOIEMENT DEV (develop branch)
  # ==========================================
  - stage: Deploy_DEV
    displayName: 'ğŸš€ DEV - DÃ©ploiement'
    dependsOn: CI_Validation
    condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/develop'))
    variables:
      - group: variable group  # Votre variable group existant
      - name: ENVIRONMENT
        value: 'DEV'
      - name: DATABRICKS_HOST
        value: 'https://adb-1119135389290183.3.azuredatabricks.net'
      - name: DATABRICKS_WORKSPACE_PATH
        value: '/Workspace/Users/mohamedazrou2003@gmail.com/DEV'
      - name: DATABRICKS_JOB_ID
        value: '811728064107146'
    jobs:
      - deployment: DeployToDev
        displayName: 'DÃ©ployer vers DEV'
        environment: 'dev'
        strategy:
          runOnce:
            deploy:
              steps:
                
                # TÃ©lÃ©charger artifact
                - download: current
                  artifact: notebooks
                  displayName: 'ğŸ“¥ TÃ©lÃ©charger artifact'
                
                # Extraire notebooks
                - task: ExtractFiles@1
                  inputs:
                    archiveFilePatterns: '$(Pipeline.Workspace)/notebooks/*.zip'
                    destinationFolder: '$(Pipeline.Workspace)/extracted'
                  displayName: 'ğŸ“¦ Extraire notebooks'
                
                # Python
                - task: UsePythonVersion@0
                  inputs:
                    versionSpec: '$(PYTHON_VERSION)'
                
                # Installer CLI
                - script: |
                    pip install databricks-cli
                    sudo apt-get update -qq && sudo apt-get install -y jq
                  displayName: 'ğŸ”§ Installer outils'
                
                # Configurer Databricks
                - script: |
                    echo "ğŸ” Configuration Databricks $(ENVIRONMENT)..."
                    
                    mkdir -p ~/.databricks
                    cat > ~/.databrickscfg <<EOF
                    [DEFAULT]
                    host = $(DATABRICKS_HOST)
                    token = $(DATABRICKS_TOKEN)
                    EOF
                    
                    # Test connexion
                    databricks workspace ls / > /dev/null 2>&1
                    if [ $? -eq 0 ]; then
                      echo "âœ… Connexion Databricks rÃ©ussie"
                    else
                      echo "âŒ Ã‰chec de connexion Databricks"
                      exit 1
                    fi
                  displayName: 'ğŸ” Configurer Databricks'
                
                # DÃ©ployer notebooks
                - script: |
                    echo "ğŸ“¤ DÃ©ploiement des notebooks vers $(ENVIRONMENT)..."
                    
                    cd $(Pipeline.Workspace)/extracted
                    
                    if [ ! -d "notebooks" ]; then
                      echo "âŒ ERREUR : Le dossier notebooks n'existe pas dans l'artifact"
                      exit 1
                    fi
                    
                    # CrÃ©er le rÃ©pertoire workspace
                    echo "ğŸ“‚ CrÃ©ation du rÃ©pertoire : $(DATABRICKS_WORKSPACE_PATH)"
                    databricks workspace mkdirs $(DATABRICKS_WORKSPACE_PATH) || echo "RÃ©pertoire existant"
                    
                    # Import des notebooks
                    echo "ğŸ“¤ Import des notebooks..."
                    databricks workspace import_dir notebooks $(DATABRICKS_WORKSPACE_PATH) --overwrite
                    
                    if [ $? -eq 0 ]; then
                      echo ""
                      echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
                      echo "   âœ… DÃ‰PLOIEMENT $(ENVIRONMENT) RÃ‰USSI"
                      echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
                      echo ""
                      echo "ğŸ“‹ Notebooks dÃ©ployÃ©s :"
                      databricks workspace ls -l $(DATABRICKS_WORKSPACE_PATH)
                      echo ""
                      echo "ğŸ”— AccÃ¨s workspace :"
                      echo "   $(DATABRICKS_HOST)/#workspace$(DATABRICKS_WORKSPACE_PATH)"
                      echo ""
                      echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
                    else
                      echo "âŒ ERREUR lors du dÃ©ploiement"
                      exit 1
                    fi
                  displayName: 'ğŸ“¤ DÃ©ployer notebooks'
                
                # VÃ©rifier synchronisation
                - script: |
                    echo "ğŸ”„ VÃ©rification de la synchronisation..."
                    
                    cd $(Pipeline.Workspace)/extracted/notebooks
                    
                    for notebook in *.py *.ipynb; do
                      if [ ! -f "$notebook" ]; then
                        continue
                      fi
                      
                      echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
                      echo "ğŸ“„ Notebook : $notebook"
                      
                      # Export depuis Databricks
                      databricks workspace export "$(DATABRICKS_WORKSPACE_PATH)/$notebook" "/tmp/$notebook" --format SOURCE 2>/dev/null
                      
                      if [ -f "/tmp/$notebook" ]; then
                        if diff -q "$notebook" "/tmp/$notebook" > /dev/null 2>&1; then
                          echo "âœ… SynchronisÃ© correctement"
                        else
                          echo "âš ï¸ DiffÃ©rences dÃ©tectÃ©es (peut Ãªtre normal)"
                        fi
                        rm -f "/tmp/$notebook"
                      else
                        echo "âš ï¸ Impossible d'exporter (vÃ©rifiez les permissions)"
                      fi
                    done
                  displayName: 'ğŸ” VÃ©rifier synchronisation'
                  continueOnError: true
                
                # ExÃ©cuter le job (si configurÃ©)
                - script: |
                    echo "ğŸš€ ExÃ©cution du job Databricks..."
                    
                    if [ -z "$(DATABRICKS_JOB_ID)" ]; then
                      echo "â„¹ï¸ Aucun job configurÃ© (DATABRICKS_JOB_ID vide)"
                      echo "ğŸ’¡ Vous pouvez exÃ©cuter les notebooks manuellement"
                      exit 0
                    fi
                    
                    echo "ğŸ¯ Job ID : $(DATABRICKS_JOB_ID)"
                    
                    # Lancer le job
                    response=$(databricks jobs run-now --job-id $(DATABRICKS_JOB_ID) 2>&1)
                    echo "ğŸ“¨ RÃ©ponse : $response"
                    
                    # Extraire run_id
                    run_id=$(echo "$response" | jq -r '.run_id // empty' 2>/dev/null)
                    
                    if [ -z "$run_id" ] || [ "$run_id" == "null" ]; then
                      echo "âŒ ERREUR : Impossible de lancer le job"
                      echo "RÃ©ponse brute : $response"
                      exit 1
                    fi
                    
                    echo "â–¶ï¸ Run ID : $run_id"
                    echo "ğŸ”— $(DATABRICKS_HOST)/#job/$(DATABRICKS_JOB_ID)/run/$run_id"
                    
                    # Attendre (avec timeout de 30 min)
                    echo "â³ Attente de la fin du job (max 30 min)..."
                    timeout 1800 databricks runs wait --run-id $run_id
                    
                    if [ $? -eq 0 ]; then
                      result_state=$(databricks runs get --run-id $run_id | jq -r '.state.result_state')
                      
                      if [ "$result_state" == "SUCCESS" ]; then
                        echo "âœ… Job terminÃ© avec succÃ¨s !"
                      else
                        echo "âŒ Job Ã©chouÃ© : $result_state"
                        exit 1
                      fi
                    else
                      echo "âš ï¸ Timeout ou erreur d'attente"
                      exit 1
                    fi
                  displayName: 'ğŸš€ ExÃ©cuter job Databricks'
                  continueOnError: true  # Ne pas bloquer si pas de job

  # ==========================================
  # STAGE 3 : DÃ‰PLOIEMENT UAT (main branch)
  # ==========================================
  - stage: Deploy_UAT
    displayName: 'ğŸ§ª UAT - DÃ©ploiement'
    dependsOn: Deploy_DEV
    condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))
    variables:
      - group: variable group
      - name: ENVIRONMENT
        value: 'UAT'
      - name: DATABRICKS_HOST
        value: 'https://adb-1119135389290183.3.azuredatabricks.net'
      - name: DATABRICKS_WORKSPACE_PATH
        value: '/Workspace/Users/mohamedazrou2003@gmail.com/UAT'
    jobs:
      - deployment: DeployToUat
        displayName: 'DÃ©ployer vers UAT'
        environment: 'uat'
        strategy:
          runOnce:
            deploy:
              steps:
                
                - download: current
                  artifact: notebooks
                
                - task: ExtractFiles@1
                  inputs:
                    archiveFilePatterns: '$(Pipeline.Workspace)/notebooks/*.zip'
                    destinationFolder: '$(Pipeline.Workspace)/extracted'
                
                - task: UsePythonVersion@0
                  inputs:
                    versionSpec: '$(PYTHON_VERSION)'
                
                - script: |
                    pip install databricks-cli
                    sudo apt-get update -qq && sudo apt-get install -y jq
                  displayName: 'ğŸ”§ Installer outils'
                
                - script: |
                    mkdir -p ~/.databricks
                    cat > ~/.databrickscfg <<EOF
                    [DEFAULT]
                    host = $(DATABRICKS_HOST)
                    token = $(DATABRICKS_TOKEN)
                    EOF
                    databricks workspace ls / > /dev/null
                  displayName: 'ğŸ” Configurer Databricks'
                
                - script: |
                    echo "ğŸ“¤ DÃ©ploiement vers $(ENVIRONMENT)..."
                    
                    cd $(Pipeline.Workspace)/extracted
                    databricks workspace mkdirs $(DATABRICKS_WORKSPACE_PATH)
                    databricks workspace import_dir notebooks $(DATABRICKS_WORKSPACE_PATH) --overwrite
                    
                    echo "âœ… DÃ©ploiement UAT terminÃ©"
                    echo "ğŸ”— $(DATABRICKS_HOST)/#workspace$(DATABRICKS_WORKSPACE_PATH)"
                  displayName: 'ğŸ“¤ DÃ©ployer vers UAT'

  # ==========================================
  # STAGE 4 : DÃ‰PLOIEMENT PROD (main + approval)
  # ==========================================
  - stage: Deploy_PROD
    displayName: 'ğŸ­ PROD - DÃ©ploiement'
    dependsOn: Deploy_UAT
    condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))
    variables:
      - group: variable group
      - name: ENVIRONMENT
        value: 'PROD'
      - name: DATABRICKS_HOST
        value: 'https://adb-1119135389290183.3.azuredatabricks.net'
      - name: DATABRICKS_WORKSPACE_PATH
        value: '/Workspace/Users/mohamedazrou2003@gmail.com/PROD'
    jobs:
      - deployment: DeployToProd
        displayName: 'DÃ©ployer vers PRODUCTION'
        environment: 'prod'  # NÃ©cessite approbation manuelle
        strategy:
          runOnce:
            deploy:
              steps:
                
                - download: current
                  artifact: notebooks
                
                - task: ExtractFiles@1
                  inputs:
                    archiveFilePatterns: '$(Pipeline.Workspace)/notebooks/*.zip'
                    destinationFolder: '$(Pipeline.Workspace)/extracted'
                
                - task: UsePythonVersion@0
                  inputs:
                    versionSpec: '$(PYTHON_VERSION)'
                
                - script: |
                    pip install databricks-cli
                    sudo apt-get update -qq && sudo apt-get install -y jq
                  displayName: 'ğŸ”§ Installer outils'
                
                - script: |
                    mkdir -p ~/.databricks
                    cat > ~/.databrickscfg <<EOF
                    [DEFAULT]
                    host = $(DATABRICKS_HOST)
                    token = $(DATABRICKS_TOKEN)
                    EOF
                    databricks workspace ls / > /dev/null
                  displayName: 'ğŸ” Configurer Databricks'
                
                - script: |
                    echo "ğŸ­ DÃ‰PLOIEMENT EN PRODUCTION..."
                    
                    cd $(Pipeline.Workspace)/extracted
                    databricks workspace mkdirs $(DATABRICKS_WORKSPACE_PATH)
                    databricks workspace import_dir notebooks $(DATABRICKS_WORKSPACE_PATH) --overwrite
                    
                    echo ""
                    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
                    echo "   ğŸ‰ DÃ‰PLOIEMENT PRODUCTION RÃ‰USSI ğŸ‰"
                    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
                    echo ""
                    echo "ğŸ“Š RÃ©sumÃ© :"
                    echo "  â€¢ Build: $(Build.BuildNumber)"
                    echo "  â€¢ Environnement: PRODUCTION"
                    echo "  â€¢ Notebooks dÃ©ployÃ©s:"
                    databricks workspace ls $(DATABRICKS_WORKSPACE_PATH)
                    echo ""
                    echo "ğŸ”— AccÃ¨s workspace :"
                    echo "   $(DATABRICKS_HOST)/#workspace$(DATABRICKS_WORKSPACE_PATH)"
                    echo ""
                    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
                  displayName: 'ğŸ­ DÃ©ployer vers PROD'