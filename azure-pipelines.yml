trigger:
  branches:
    include:
      - main
  paths:
    include:
      - notebooks/**

pool:
  vmImage: 'ubuntu-latest'

variables:
  - group: variable group
  - name: DATABRICKS_HOST
    value: 'https://adb-1119135389290183.3.azuredatabricks.net'
  - name: DATABRICKS_WORKSPACE_PATH
    value: '/Workspace/Users/mohamedazrou2003@gmail.com'
  - name: DATABRICKS_JOB_ID
    value: '811728064107146' # ID r√©el de ton job Databricks

steps:
# 1Ô∏è‚É£ DEBUG: v√©rifier le repo et les notebooks
- script: |
    echo "üìÇ STRUCTURE DU REPOSITORY:"
    ls -R
    echo ""
    echo "üìÅ Contenu du dossier notebooks:"
    ls -la notebooks/ || echo "‚ùå Le dossier notebooks n'existe pas!"
  displayName: 'üîç DEBUG - Structure du repo'

- script: |
    echo "üîç COMMITS R√âCENTS:"
    git log --oneline -5
    echo ""
    echo "üìù FICHIERS MODIFI√âS DANS LE DERNIER COMMIT:"
    git diff --name-only HEAD~1 HEAD || git ls-files
    echo ""
    echo "üéØ FICHIERS NOTEBOOKS MODIFI√âS:"
    git diff --name-only HEAD~1 HEAD | grep "^notebooks/" || echo "Aucun notebook modifi√©"
  displayName: 'üîç DEBUG - Changements d√©tect√©s'

# 2Ô∏è‚É£ Installer Python
- task: UsePythonVersion@0
  inputs:
    versionSpec: '3.10'
  displayName: 'Installer Python 3.10'

# 3Ô∏è‚É£ Installer d√©pendances
- script: |
    pip install -r requirements.txt
    pip install databricks-cli
  displayName: 'Installer d√©pendances'

# 4Ô∏è‚É£ Configurer Databricks CLI
- script: |
    cat > ~/.databrickscfg <<EOF
    [DEFAULT]
    host = $(DATABRICKS_HOST)
    token = $(DATABRICKS_TOKEN)
    EOF

    echo "‚úÖ CLI Databricks configur√©e"
    echo "üîå Test de connexion au workspace :"
    databricks workspace ls /Workspace/Users || echo "‚ùå Connexion √©chou√©e"
    databricks --version
  displayName: 'Configurer Databricks CLI'

# 5Ô∏è‚É£ D√©ployer tous les notebooks
- script: |
    echo "üì§ D√©ploiement des notebooks..."
    if [ ! -d "notebooks" ]; then
      echo "‚ùå ERREUR: Le dossier 'notebooks' n'existe pas!"
      exit 1
    fi

    databricks workspace import_dir notebooks $(DATABRICKS_WORKSPACE_PATH) --overwrite

    if [ $? -eq 0 ]; then
      echo "‚úÖ D√©ploiement r√©ussi!"
      databricks workspace ls -l $(DATABRICKS_WORKSPACE_PATH)
    else
      echo "‚ùå √âchec du d√©ploiement"
      exit 1
    fi
  displayName: 'D√©ployer notebooks vers Databricks'

# 6Ô∏è‚É£ Comparer les notebooks
- script: |
    echo "üîÑ Comparaison des notebooks..."
    for notebook in notebooks/*.py notebooks/*.ipynb; do
      if [ -f "$notebook" ]; then
        filename=$(basename "$notebook")
        echo "üìÑ Notebook local: $filename"
        echo "Hash MD5 local: $(md5sum "$notebook" | cut -d' ' -f1)"

        databricks workspace export $(DATABRICKS_WORKSPACE_PATH)/$filename /tmp/$filename --format SOURCE 2>/dev/null
        if [ -f "/tmp/$filename" ]; then
          echo "Hash MD5 Databricks: $(md5sum /tmp/$filename | cut -d' ' -f1)"
          if diff "$notebook" "/tmp/$filename" > /dev/null; then
            echo "‚úÖ Identique"
          else
            echo "‚ö†Ô∏è Diff√©rences d√©tect√©es"
            diff "$notebook" "/tmp/$filename" | head -20
          fi
        else
          echo "‚ö†Ô∏è Impossible de t√©l√©charger depuis Databricks"
        fi
        echo "---"
      fi
    done
  displayName: 'üîç V√©rifier synchronisation'

# 7Ô∏è‚É£ Installer jq pour parser JSON
- script: |
    sudo apt-get update
    sudo apt-get install -y jq
  displayName: 'Installer jq (parser JSON)'

# 8Ô∏è‚É£ Ex√©cuter le job Databricks et attendre sa fin
- script: |
    echo "üöÄ Ex√©cution du job Databricks ID=$(DATABRICKS_JOB_ID)"
    run_id=$(databricks jobs run-now --job-id $(DATABRICKS_JOB_ID) | jq -r '.run_id')
    if [ -z "$run_id" ]; then
      echo "‚ùå ERREUR: Impossible de r√©cup√©rer run_id !"
      exit 1
    fi
    echo "‚ñ∂Ô∏è Run ID : $run_id"
    echo "‚è≥ En attente de la fin du job..."
    databricks runs wait --run-id $run_id
    echo "‚úÖ Job Databricks termin√© avec succ√®s!"
  displayName: 'üöÄ Ex√©cuter job Databricks apr√®s d√©ploiement'
