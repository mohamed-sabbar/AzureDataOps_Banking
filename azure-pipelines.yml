trigger:
  - main

pool:
  vmImage: 'ubuntu-latest'

variables:
  DATABRICKS_HOST: 'https://adb-xxxxx.azuredatabricks.net'  # Remplacez par votre URL Databricks
  DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)  # Token stock√© dans Azure DevOps comme variable secr√®te
  JOB_ID: '811728064107146'  # Votre ID de job existant

stages:
  - stage: DeployAndRun
    displayName: 'üöÄ D√©ployer et Ex√©cuter Job Databricks'
    jobs:
      - job: DatabricksJob
        displayName: 'Job Databricks'
        steps:
          
          # √âtape 1 : Installation et configuration
          - task: Bash@3
            displayName: 'üîß Installation Databricks CLI'
            inputs:
              targetType: 'inline'
              script: |
                echo "Installation de Databricks CLI..."
                pip install databricks-cli
                databricks --version
          
          # √âtape 2 : Configuration de l'authentification
          - task: Bash@3
            displayName: 'üîê Configuration authentification'
            inputs:
              targetType: 'inline'
              script: |
                echo "Configuration de Databricks CLI..."
                
                # Cr√©er le fichier de configuration
                mkdir -p ~/.databrickscfg
                cat <<EOF > ~/.databrickscfg
                [DEFAULT]
                host = $(DATABRICKS_HOST)
                token = $(DATABRICKS_TOKEN)
                EOF
                
                # Configurer pour utiliser Jobs API 2.1
                databricks jobs configure --version=2.1
                
                echo "‚úÖ Configuration termin√©e"
          
          # √âtape 3 : V√©rifier/Cr√©er la configuration du job
          - task: Bash@3
            displayName: 'üìù V√©rifier configuration du job'
            inputs:
              targetType: 'inline'
              script: |
                echo "V√©rification du job $(JOB_ID)..."
                
                # V√©rifier si le job existe
                JOB_INFO=$(databricks jobs get --job-id $(JOB_ID) 2>&1)
                
                if echo "$JOB_INFO" | grep -q "RESOURCE_DOES_NOT_EXIST"; then
                  echo "‚ö†Ô∏è Le job n'existe pas, cr√©ation d'un nouveau job..."
                  
                  # Cr√©er la configuration du job
                  cat <<EOF > job_config.json
                  {
                    "name": "Mon Job Databricks - Pipeline",
                    "tasks": [
                      {
                        "task_key": "notebook_task",
                        "notebook_task": {
                          "notebook_path": "/Users/votre@email.com/mon_notebook",
                          "base_parameters": {
                            "env": "production",
                            "date": "$(Build.BuildNumber)"
                          }
                        },
                        "new_cluster": {
                          "spark_version": "13.3.x-scala2.12",
                          "node_type_id": "Standard_DS3_v2",
                          "num_workers": 2,
                          "spark_conf": {
                            "spark.databricks.delta.preview.enabled": "true"
                          }
                        },
                        "timeout_seconds": 3600,
                        "max_retries": 2
                      }
                    ],
                    "timeout_seconds": 7200,
                    "max_concurrent_runs": 1
                  }
                  EOF
                  
                  # Cr√©er le job
                  NEW_JOB=$(databricks jobs create --json-file job_config.json)
                  NEW_JOB_ID=$(echo $NEW_JOB | jq -r '.job_id')
                  echo "‚úÖ Nouveau job cr√©√© avec l'ID: $NEW_JOB_ID"
                  echo "##vso[task.setvariable variable=JOB_ID]$NEW_JOB_ID"
                  
                else
                  echo "‚úÖ Job $(JOB_ID) existe d√©j√†"
                  
                  # V√©rifier si le job a des t√¢ches
                  TASKS=$(echo "$JOB_INFO" | jq -r '.settings.tasks // empty')
                  
                  if [ -z "$TASKS" ] || [ "$TASKS" = "null" ]; then
                    echo "‚ö†Ô∏è Le job n'a pas de t√¢ches configur√©es, mise √† jour..."
                    
                    # Cr√©er la configuration avec t√¢ches
                    cat <<EOF > job_update.json
                    {
                      "job_id": $(JOB_ID),
                      "new_settings": {
                        "name": "Mon Job Databricks - Pipeline Updated",
                        "tasks": [
                          {
                            "task_key": "notebook_task",
                            "notebook_task": {
                              "notebook_path": "/Users/votre@email.com/mon_notebook",
                              "base_parameters": {
                                "env": "production",
                                "date": "$(Build.BuildNumber)"
                              }
                            },
                            "new_cluster": {
                              "spark_version": "13.3.x-scala2.12",
                              "node_type_id": "Standard_DS3_v2",
                              "num_workers": 2
                            },
                            "timeout_seconds": 3600
                          }
                        ],
                        "timeout_seconds": 7200,
                        "max_concurrent_runs": 1
                      }
                    }
                    EOF
                    
                    # Mettre √† jour le job
                    databricks jobs reset --json-file job_update.json
                    echo "‚úÖ Job mis √† jour avec succ√®s"
                  fi
                fi
          
          # √âtape 4 : Lancer le job
          - task: Bash@3
            displayName: 'üöÄ Ex√©cuter job Databricks'
            inputs:
              targetType: 'inline'
              script: |
                echo "üöÄ Lancement du job $(JOB_ID)..."
                
                # Lancer le job
                RUN_OUTPUT=$(databricks jobs run-now --job-id $(JOB_ID) 2>&1)
                echo "üì® R√©ponse du lancement:"
                echo "$RUN_OUTPUT"
                
                # Extraire le run_id
                RUN_ID=$(echo "$RUN_OUTPUT" | jq -r '.run_id // empty' 2>/dev/null)
                
                if [ -z "$RUN_ID" ] || [ "$RUN_ID" = "null" ]; then
                  echo "‚ùå ERREUR: Impossible de r√©cup√©rer le run_id"
                  echo "R√©ponse compl√®te: $RUN_OUTPUT"
                  exit 1
                fi
                
                echo "‚úÖ Job lanc√© avec succ√®s!"
                echo "üìä Run ID: $RUN_ID"
                echo "üîó URL: $(DATABRICKS_HOST)/#job/$(JOB_ID)/run/$RUN_ID"
                
                # Sauvegarder le run_id pour les √©tapes suivantes
                echo "##vso[task.setvariable variable=RUN_ID]$RUN_ID"
          
          # √âtape 5 : Surveiller l'ex√©cution (optionnel)
          - task: Bash@3
            displayName: '‚è≥ Attendre fin d\'ex√©cution'
            inputs:
              targetType: 'inline'
              script: |
                echo "‚è≥ Surveillance de l'ex√©cution du job..."
                
                MAX_WAIT=3600  # 1 heure maximum
                ELAPSED=0
                SLEEP_TIME=30
                
                while [ $ELAPSED -lt $MAX_WAIT ]; do
                  # Obtenir le statut
                  RUN_STATUS=$(databricks runs get --run-id $(RUN_ID) | jq -r '.state.life_cycle_state')
                  RESULT_STATE=$(databricks runs get --run-id $(RUN_ID) | jq -r '.state.result_state // "RUNNING"')
                  
                  echo "üìä Statut: $RUN_STATUS - R√©sultat: $RESULT_STATE"
                  
                  # V√©rifier si termin√©
                  if [ "$RUN_STATUS" = "TERMINATED" ] || [ "$RUN_STATUS" = "SKIPPED" ]; then
                    if [ "$RESULT_STATE" = "SUCCESS" ]; then
                      echo "‚úÖ Job termin√© avec succ√®s!"
                      exit 0
                    else
                      echo "‚ùå Job √©chou√© avec le statut: $RESULT_STATE"
                      databricks runs get-output --run-id $(RUN_ID)
                      exit 1
                    fi
                  fi
                  
                  # Attendre avant la prochaine v√©rification
                  sleep $SLEEP_TIME
                  ELAPSED=$((ELAPSED + SLEEP_TIME))
                done
                
                echo "‚ö†Ô∏è Timeout atteint apr√®s $MAX_WAIT secondes"
                exit 1
            condition: succeededOrFailed()
          
          # √âtape 6 : R√©cup√©rer les logs (optionnel)
          - task: Bash@3
            displayName: 'üìã R√©cup√©rer logs'
            inputs:
              targetType: 'inline'
              script: |
                echo "üìã R√©cup√©ration des logs..."
                databricks runs get-output --run-id $(RUN_ID)
            condition: always()